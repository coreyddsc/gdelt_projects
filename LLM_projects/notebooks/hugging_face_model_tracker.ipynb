{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3bb0d4",
   "metadata": {},
   "source": [
    "# Hugging Face Cached Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd110e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# GPU Timing (using GPU 1)\n",
    "device_id = 1 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 XL (1.5B parameters)\n",
    "# Initialize GPT-2 XL text generation pipeline\n",
    "pipe_gpt2xl = pipeline('text-generation', model='gpt2-xl')\n",
    "# Example usage\n",
    "output_gpt2xl = pipe_gpt2xl(\"In a world where technology rules,\", max_length=50, truncation=True, num_return_sequences=1)\n",
    "print(output_gpt2xl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172d5df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Once upon a time, deep in the forest, it was never known whether or not the old woman was still living. To the children—all but the youngest—she was no more than a vague apparition, a vague sound of crumpling'}]\n"
     ]
    }
   ],
   "source": [
    "# GPT-Neo (2.7B parameters)\n",
    "# Initialize GPT-Neo (2.7B) text generation pipeline\n",
    "pipe_gptneo = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B', device=device_id)\n",
    "\n",
    "# Example usage\n",
    "output_gptneo = pipe_gptneo(\"Once upon a time, deep in the forest,\", max_length=50, truncation=True, num_return_sequences=1)\n",
    "print(output_gptneo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f846eb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-J (6B parameters)\n",
    "# Initialize GPT-J (6B) text generation pipeline\n",
    "pipe_gptj = pipeline('text-generation', model='EleutherAI/gpt-j-6B', device=device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "output_gptj = pipe_gptj(\"The mysteries of the universe began to unravel when,\", max_length=50, truncation=True, num_return_sequences=1)\n",
    "print(output_gptj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71984a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOOM (7B parameters)\n",
    "# Initialize BLOOM-7B text generation pipeline\n",
    "pipe_bloom7b = pipeline('text-generation', model='bigscience/bloom-7b1', device=device_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcaf7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "output_bloom7b = pipe_bloom7b(\"At the dawn of the AI age,\", max_length=50, truncation=True, num_return_sequences=1)\n",
    "print(output_bloom7b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cdd6ba",
   "metadata": {},
   "source": [
    "# Utilize a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d16824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import pipeline, set_seed\n",
    "import torch\n",
    "set_seed(42)\n",
    "# Text for testing\n",
    "input_text = \"In a world where technology rules,\"\n",
    "\n",
    "# GPU Timing (using GPU 1)\n",
    "device_id = 1 if torch.cuda.is_available() else -1\n",
    "pipe_gpt2xl_gpu = pipeline('text-generation', model='gpt2-xl', device=device_id)\n",
    "\n",
    "start_time_gpu = time.time()  # Start the timer\n",
    "output_gpu = pipe_gpt2xl_gpu(input_text, max_length=50, truncation=True, num_return_sequences=1)\n",
    "end_time_gpu = time.time()    # End the timer\n",
    "\n",
    "print(\"GPU output:\", output_gpu)\n",
    "print(f\"GPU processing time: {end_time_gpu - start_time_gpu:.2f} seconds\")\n",
    "\n",
    "\n",
    "# CPU Timing (forcing CPU usage)\n",
    "pipe_gpt2xl_cpu = pipeline('text-generation', model='gpt2-xl', device=-1)\n",
    "\n",
    "start_time_cpu = time.time()  # Start the timer\n",
    "output_cpu = pipe_gpt2xl_cpu(input_text, max_length=50, truncation=True, num_return_sequences=1)\n",
    "end_time_cpu = time.time()    # End the timer\n",
    "\n",
    "print(\"CPU output:\", output_cpu)\n",
    "print(f\"CPU processing time: {end_time_cpu - start_time_cpu:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "500f4789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Oct 29 11:08:33 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA RTX A6000               On  | 00000000:2E:00.0 Off |                  Off |\r\n",
      "| 30%   31C    P8              27W / 300W |  17062MiB / 49140MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "|   1  NVIDIA RTX A6000               On  | 00000000:41:00.0 Off |                  Off |\r\n",
      "| 30%   37C    P2              78W / 300W |  10782MiB / 49140MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A      2162      G   /usr/lib/xorg/Xorg                            4MiB |\r\n",
      "|    0   N/A  N/A     34515      C   /opt/conda/bin/python3.10                 17046MiB |\r\n",
      "|    1   N/A  N/A      2162      G   /usr/lib/xorg/Xorg                            4MiB |\r\n",
      "|    1   N/A  N/A    237895      C   ...tebooks/gdelt/gdelt-venv/bin/python    10766MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi # GPU status and available memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5829946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kill -9 237895 # if too many processes are going on a GPU, you can find your PID number and stop the process. Freeing memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py-gdelt",
   "language": "python",
   "name": "py-gdelt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
